# ğŸ§Š E-Commerce Sales Analytics Pipeline using Snowflake

This project is a full-fledged ELT pipeline built on **Snowflake**, designed to process e-commerce sales data from raw S3 files through Bronze, Silver, and Gold layers â€” following modern data engineering best practices.

## Architecture

![image](https://github.com/user-attachments/assets/4ddacc03-380c-447a-93a1-a6fb7286cf77)

![image](https://github.com/user-attachments/assets/48253309-65c5-4220-86f6-e4e33ed9d149)

---

## ğŸ› ï¸ Tech Stack

- **Snowflake** (Data Warehouse)
- **AWS S3** (Raw Data Storage)
- **Snowpipe** (Continuous data ingestion)
- **Streams & Tasks** (Incremental loading & scheduling)
- **SQL Stored Procedures** (Reusable logic layers)

---

## ğŸ§ª What This Project Does

1. Ingests raw CSV data from Amazon S3 into Snowflake via Snowpipe
2. Cleans and transforms the data through Bronze â†’ Silver â†’ Gold layers
3. Automates incremental updates using **Streams** and **Tasks**
4. Generates customer-level metrics like **total orders, total spend**, and **customer type**

---

## ğŸ“‚ Data Architecture

### ğŸŸ¤ Bronze Layer
- Ingests raw CSV files into external stages and loads into staging tables
- Uses **Snowpipe** with SQS event notification to auto-ingest data on arrival
- Example: `bronze.customers`, `bronze.orders`, etc.

### ğŸŸ  Silver Layer
- Applies cleaning and normalization logic
- Uses **Streams** on bronze tables and a stored procedure `sp_run_silver()`
- Automates transformation using a **scheduled Task** that runs every 1 minute
- Output: `silver.cleaned_customers`, `silver.cleaned_orders`, etc.

### ğŸŸ¡ Gold Layer
- Aggregates data for analytics and reporting
- Created as a materialized **table** via `sp_run_gold()` procedure
- Optional: Can be automated using a chained Task (`AFTER task_stream_has_data`)
- Output: `gold.customer_order_stats`

---

## ğŸ§° Components Walkthrough

![image](https://github.com/user-attachments/assets/3a4d1163-97d8-417a-8fb6-b87c336557b1)


### âœ… 1. ğŸ“‚ one_time_setup/
This folder contains one-time initialization scripts for setting up a Snowflake-based ELT pipeline with S3 as the data source and a multi-layer architecture: Bronze â†’ Silver â†’ Gold.

#### setup.sql
ğŸ§© Purpose: Initializes the Snowflake environment for the ELT pipeline
âœ… Actions:
* Creates the project database: alpha_db
* Sets up three schemas for the layered architecture: bronze, silver, and gold
* Defines raw data tables in the bronze schema:
* customers, orders, order_items, payments, products
* Creates a reusable CSV FILE FORMAT for ingesting data from S3
* Establishes a secure STORAGE INTEGRATION with AWS using IAM role access

#### storage_integration_setup.sql
ğŸ§© Purpose: Establishes secure data ingestion from S3 to Snowflake's Bronze layer
âœ… Actions:
* Uses the previously created STORAGE INTEGRATION (ecommerce_s3_integration) to securely access S3 data
* Creates external stages for each raw data folder in S3:
  - s3_stage_customers, s3_stage_order_items, s3_stage_orders, s3_stage_payments, s3_stage_products
* Defines Snowpipes with AUTO_INGEST = TRUE to automatically load new S3 files into Bronze tables
* Each pipe targets a corresponding table in the Bronze schema (customers, orders, etc.)
* Uses ON_ERROR = 'CONTINUE' to skip corrupt rows and ensure ingestion continuity

#### stream_bronze_to_silver.sql
ğŸ§© Purpose: Tracks new data in Bronze tables for incremental processing
âœ… Actions:
* Creates Streams on Bronze tables to capture INSERT operations
* Enables change data capture (CDC) for:
  - customers, orders, payments, order_items, products
* Used in the Silver-layer procedure for incremental loading.

#### test_stream_has_data.sql
ğŸ§© Purpose: Validates if the stream has captured new data
âœ… Actions:
* Queries stream_customers to inspect incoming records from S3
* Helps verify if Snowpipe ingestion and stream capture are working correctly

#### stream_silver_to_gold.sql
ğŸ§© Purpose: Enables incremental loading from Silver to Gold layer
âœ… Actions:
* Creates Streams on Silver tables to capture INSERT operations
* Streams created on:
* customers, orders, payments, order_items, products
* Used for building Gold aggregations incrementally.


---

### âœ… 2. ğŸ“‚ bronze/

![image](https://github.com/user-attachments/assets/3df4d473-05a4-4aac-bd48-6909f25256ad)

#### test_customers_table.sql
ğŸ§© Purpose: Validates raw customer data in the Bronze layer
âœ… Actions:
* Runs a simple SELECT * FROM customers in the bronze schema
* Verifies that data ingestion via Snowpipe was successful

---

### âœ… 3. ğŸ“‚ silver/

![image](https://github.com/user-attachments/assets/a3e93dc2-ba22-4b1d-92c6-39d55c125cd6)



## ğŸ§  Key Concepts Learned
Real-time ingestion with Snowpipe + SQS

Incremental ELT using Streams + Tasks

End-to-end automation via stored procedures

Monitoring via TASK_HISTORY() and CLI

Python integration for advanced control

---

## ğŸ‘¨â€ğŸ’» Author

Made with â¤ï¸ by Anuj Kothiyal
Cloud & Data Engineering Enthusiast

